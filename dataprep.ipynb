{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import time\n",
    "from dicom_reader import DicomReader\n",
    "from registration import SitkRegister\n",
    "from applyTransform import apply_transform\n",
    "import os\n",
    "\n",
    "def path_prep(rootpath, patient_ID):\n",
    "    patient_ID = str(patient_ID)\n",
    "    ct_path = []\n",
    "    pet_path = []\n",
    "    print(os.path.join(rootpath ,\"P\"+patient_ID, \"p\" + patient_ID + \"t1\", \"p\" + patient_ID + \"t1pet1\"))\n",
    "    if os.path.exists(os.path.join(rootpath ,\"P\"+patient_ID, \"p\" + patient_ID + \"t1\", \"p\" + patient_ID + \"t1pet1\")):\n",
    "        ct_path.append(os.path.join(rootpath ,\"P\"+patient_ID,\"p\" + patient_ID + \"t1\", \"p\" + patient_ID + \"t1ct1\"))\n",
    "        pet_path.append(os.path.join(rootpath ,\"P\"+patient_ID,\"p\" + patient_ID + \"t1\", \"p\" + patient_ID + \"t1pet1\"))\n",
    "    if os.path.exists(os.path.join(rootpath ,\"P\"+patient_ID,\"p\" + patient_ID + \"t1\", \"p\" + patient_ID + \"t1pet2\")):\n",
    "        ct_path.append(os.path.join(rootpath ,\"P\"+patient_ID,\"p\" + patient_ID + \"t1\", \"p\" + patient_ID + \"t1ct2\"))\n",
    "        pet_path.append(os.path.join(rootpath ,\"P\"+patient_ID,\"p\" + patient_ID + \"t1\", \"p\" + patient_ID + \"t1pet2\"))\n",
    "    if os.path.exists(os.path.join(rootpath ,\"P\"+patient_ID,\"p\" + patient_ID + \"t2\", \"p\" + patient_ID + \"t2pet1\")):\n",
    "        ct_path.append(os.path.join(rootpath ,\"P\"+patient_ID,\"p\" + patient_ID + \"t2\", \"p\" + patient_ID + \"t2ct1\"))\n",
    "        pet_path.append(os.path.join(rootpath ,\"P\"+patient_ID,\"p\" + patient_ID + \"t2\", \"p\" + patient_ID + \"t2pet1\"))\n",
    "\n",
    "    return ct_path, pet_path\n",
    "\n",
    "def path_prep_reg(rootpath, patient_ID):\n",
    "    patient_ID = str(patient_ID)\n",
    "    ct_path = []\n",
    "    pet_path = []\n",
    "    if os.path.exists(os.path.join(rootpath ,\"P\"+patient_ID,\"p\" + patient_ID + \"t1\", \"p\" + patient_ID + \"t1pet1\")):\n",
    "        ct_path.append(os.path.join(rootpath ,\"P\"+patient_ID,\"p\" + patient_ID + \"t1\", \"p\" + patient_ID + \"t1ct1\"))\n",
    "        pet_path.append(os.path.join(rootpath ,\"p\" + patient_ID + \"t1\", \"p\" + patient_ID + \"t1pet1\"))\n",
    "    if os.path.exists(os.path.join(rootpath ,\"P\"+patient_ID,\"p\" + patient_ID + \"t1\", \"p\" + patient_ID + \"t1pet2\")):\n",
    "        ct_path.append(os.path.join(rootpath ,\"P\"+patient_ID,\"p\" + patient_ID + \"t1\", \"p\" + patient_ID + \"t1ct2\"))\n",
    "        pet_path.append(os.path.join(rootpath ,\"P\"+patient_ID,\"p\" + patient_ID + \"t1\", \"p\" + patient_ID + \"t1pet2\"))\n",
    "    if os.path.exists(os.path.join(rootpath ,\"P\"+patient_ID,\"p\" + patient_ID + \"t2\", \"p\" + patient_ID + \"t2pet1\")):\n",
    "        ct_path.append(os.path.join(rootpath ,\"P\"+patient_ID,\"p\" + patient_ID + \"t2\", \"p\" + patient_ID + \"t2ct1\"))\n",
    "        pet_path.append(os.path.join(rootpath ,\"P\"+patient_ID,\"p\" + patient_ID + \"t2\", \"p\" + patient_ID + \"t2pet1\"))\n",
    "\n",
    "    return ct_path, pet_path\n",
    "\n",
    "def finetune_volume(rootpath, patient_ID, timepoint,  threshold, cropsize, shift, translation, flipct):\n",
    "    # timepoint: 0: t1ct1, t1pet1, 1: t1ct2, t1pet2, 2: t2ct1, t2pet2\n",
    "    ct_path, pet_path = path_prep(rootpath, patient_ID)\n",
    "    dicomreader = DicomReader(ct_path[timepoint], pet_path[timepoint], threshold, cropsize, shift, translation, flipct)\n",
    "    dicomreader.save_dicom()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Samsung_T5/P15/p15t1/p15t1pet1\n",
      "working on:/Volumes/Samsung_T5/P15/p15t1/p15t1ct1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[33m2022-09-07 11:20:18.643 (3707.134s) [          2D9EFE]vtkOpenGLVolumeLookupTa:87    WARN| vtkOpenGLVolumeOpacityTable (0x7fa25832cf50): This OpenGL implementation does not support the required texture size of 131072, falling back to maximum allowed, 16384.This may cause an incorrect lookup table mapping.\u001b[0m\n",
      "\u001b[0m\u001b[33m2022-09-07 11:20:18.643 (3707.135s) [          2D9EFE]vtkOpenGLVolumeLookupTa:87    WARN| vtkOpenGLVolumeRGBTable (0x7fa25832ce80): This OpenGL implementation does not support the required texture size of 131072, falling back to maximum allowed, 16384.This may cause an incorrect lookup table mapping.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e93c9e756a64ff3a1f6cc161c8f261c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ViewInteractiveWidget(height=768, layout=Layout(height='auto', width='100%'), width=1024)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on:/Volumes/Samsung_T5/P15/p15t2/p15t2ct1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[33m2022-09-07 11:21:43.613 (3792.102s) [          2D9EFE]vtkOpenGLVolumeLookupTa:87    WARN| vtkOpenGLVolumeOpacityTable (0x7fa2b649d1e0): This OpenGL implementation does not support the required texture size of 262144, falling back to maximum allowed, 16384.This may cause an incorrect lookup table mapping.\u001b[0m\n",
      "\u001b[0m\u001b[33m2022-09-07 11:21:43.614 (3792.103s) [          2D9EFE]vtkOpenGLVolumeLookupTa:87    WARN| vtkOpenGLVolumeRGBTable (0x7fa2b649d350): This OpenGL implementation does not support the required texture size of 262144, falling back to maximum allowed, 16384.This may cause an incorrect lookup table mapping.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51eef4a510e4401eb7a8354cd582d2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ViewInteractiveWidget(height=768, layout=Layout(height='auto', width='100%'), width=1024)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rootpath = os.path.dirname(os.getcwd())\n",
    "patient_ID_list = [ 15]\n",
    "\n",
    "for patient_ID in  patient_ID_list:\n",
    "    ct_path, pet_path = path_prep(rootpath, patient_ID)\n",
    "\n",
    "    for i in range(len(ct_path)):\n",
    "        print('working on:' + ct_path[i])\n",
    "        dicomreader = DicomReader(ct_path[i], pet_path[i])\n",
    "        dicomreader.save_dicom()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath = os.path.join(os.path.dirname(os.getcwd()))\n",
    "finetune_volume(rootpath, 16, 2,  200, (120, 120, 240), (0, 20, 20),(20, 0, 0), False) # move VOI up for patient 4 timepoint 2\n",
    "#finetune_volume(rootpath, 4, 2,  200, (120, 120, 240), (0, 20, 60),(0, 0, 0)) # move VOI up for patient 4 timepoint 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Register now\n",
    "rootpath = os.path.join(os.path.dirname(os.getcwd()),'python')\n",
    "patient_ID_list = [5]\n",
    "for patient_ID in  patient_ID_list:\n",
    "    ct_path, pet_path = path_prep_reg(rootpath, patient_ID)\n",
    "    fixed_file_ct = ct_path[0]\n",
    "    moving_file_ct1 = ct_path[1]\n",
    "    moving_file_ct2 = ct_path[2]\n",
    "\n",
    "    fixed_file_pet = pet_path[0]\n",
    "    moving_file_pet1 = pet_path[1]\n",
    "    moving_file_pet2 = pet_path[2]\n",
    "    \n",
    "    # register t1\n",
    "    new_register1 = SitkRegister(fixed_file_ct, moving_file_ct1)\n",
    "    new_register1.register()\n",
    "    new_register1.plotRegister_result()\n",
    "\n",
    "    \n",
    "\n",
    "    # register t2\n",
    "    new_register1 = SitkRegister(fixed_file_ct, moving_file_ct2)\n",
    "    new_register1.register()\n",
    "    new_register1.plotRegister_result()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transform\n",
    "rootpath = os.path.join(os.path.dirname(os.getcwd()),'python')\n",
    "patient_ID_list = [5]\n",
    "# patient_ID_list = [14,20]\n",
    "for patient_ID in  patient_ID_list:\n",
    "    ct_path, pet_path = path_prep_reg(rootpath, patient_ID)\n",
    "  \n",
    "    fixed_file_pet = pet_path[0]\n",
    "    moving_file_pet1 = pet_path[1]\n",
    "    moving_file_pet2 = pet_path[2]\n",
    "\n",
    "    fixed_file_ct = ct_path[0]\n",
    "    moving_file_ct1 = ct_path[1]\n",
    "    moving_file_ct2 = ct_path[2]\n",
    "    \n",
    "    split_moving_path = os.path.normpath(moving_file_pet1).split(os.path.sep)\n",
    "    split_moving_ct_path = os.path.normpath(moving_file_ct1).split(os.path.sep)\n",
    " \n",
    "    \n",
    "    transform_path= os.path.join(rootpath,split_moving_path[-2],split_moving_ct_path[-1])+'transform.tfm'\n",
    "    \n",
    "    apply_transform(transform_path, fixed_file_pet, moving_file_pet1)\n",
    "    split_moving_path = os.path.normpath(moving_file_pet2).split(os.path.sep)\n",
    "    split_moving_ct_path = os.path.normpath(moving_file_ct2).split(os.path.sep)\n",
    "    transform_path = os.path.join(rootpath,split_moving_path[-2], split_moving_ct_path[-1])+'transform.tfm'\n",
    "    apply_transform(transform_path, fixed_file_pet, moving_file_pet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center(img, cropsize, shift):\n",
    "        (cropx, cropy, cropz) = cropsize\n",
    "        (shiftx, shifty, shiftz) = shift\n",
    "        y,x,z = img.shape\n",
    "        startx = x//2-(cropx//2)\n",
    "        starty = y//2-(cropy//2)\n",
    "        startz = z//2-(cropz//2)      \n",
    "        return img[starty+shifty:starty+cropy+shifty,startx+shiftx:startx+cropx+shiftx, startz+shiftz:startz+cropz+shiftz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import util\n",
    "\n",
    "def make_blocks_vectorized(x,d):\n",
    "    # d: chuck size\n",
    "    p,m,n = x.shape\n",
    "    return x.reshape(-1,m//d,d,n//d,d).transpose(1,3,0,2,4).reshape(-1,p,d,d) # shape: number of chuncks, x_dimention, d, d\n",
    "\n",
    "def unmake_blocks_vectorized(x,d,m,n):    \n",
    "    return np.concatenate(x).reshape(m//d,n//d,d,d).transpose(0,2,1,3).reshape(m,n)\n",
    "\n",
    "def path_prep_data(rootpath, patient_ID):\n",
    "    patient_ID = str(patient_ID)\n",
    "    t1_path = os.path.join(rootpath ,\"p\" + patient_ID + \"t1\", \"p\" + patient_ID + \"t1pet1_raw\")\n",
    "    #t1_path = os.path.join(rootpath ,\"p\" + patient_ID + \"t1\", \"p\" + patient_ID + \"t1pet1_norm\")\n",
    "    t2_path = os.path.join(rootpath ,\"p\" + patient_ID + \"t1\", \"p\" + patient_ID + \"t1pet2_raw_t\")\n",
    "    t3_path = os.path.join(rootpath ,\"p\" + patient_ID + \"t2\", \"p\" + patient_ID + \"t2pet1_raw_t\")\n",
    "    idx_path = os.path.join(rootpath ,\"p\" + patient_ID + \"t1\", \"p\" + patient_ID + \"t1ct1_raw\")\n",
    "\n",
    "    return t1_path, t2_path, t3_path, idx_path\n",
    "\n",
    "t1_data = []\n",
    "t2_data = []\n",
    "t3_data = []\n",
    "idx_data = []\n",
    "\n",
    "#patient_ID_list = [1,3,5,7,9,11,14,17, 19,21] # for training\n",
    "#patient_ID_list = [2,4,6,8,10,13,16,18,20] # for testing\n",
    "patient_ID_list = [4,6,10,1,16,17,18,13]\n",
    "# patient_ID_list = [4,1,6,17]\n",
    "rootpath = os.path.join(os.path.dirname(os.getcwd()),'python')\n",
    "for patient_ID in  patient_ID_list:\n",
    "    t1_path, t2_path, t3_path, idx_path = path_prep_data(rootpath, patient_ID)\n",
    "\n",
    "    t1 = np.load(t1_path + '.npy')\n",
    "    t2 = np.load(t2_path + '.npy')\n",
    "    t3 = np.load(t3_path + '.npy')\n",
    "    idx = np.load(idx_path + '.npy')\n",
    "\n",
    "\n",
    "    # t1[t1<50] =0\n",
    "    # t2[t2<50] =0\n",
    "    # t3[t3<50] =0\n",
    "\n",
    "\n",
    "    idx = (idx > 200)\n",
    "    idx = idx.astype(int)\n",
    "\n",
    "    \n",
    "    t1[t1<100] =0\n",
    "    t2[t2<100] =0\n",
    "    t3[t3<100] =0\n",
    "\n",
    "    t1 = t1/np.sum(t1)*1e7\n",
    "    t2 = t2/np.sum(t2)*1e7\n",
    "    t3 = t3/np.sum(t3)*1e7\n",
    "\n",
    "\n",
    "    # t1 = util.normalize_3d_meanstd(t1)\n",
    "    # t2 = util.normalize_3d_meanstd(t2)\n",
    "    # t3 = util.normalize_3d_meanstd(t3)\n",
    "\n",
    "    # t1 = util.normalize_3d_minmax(t1)\n",
    "    # t2 = util.normalize_3d_minmax(t2)\n",
    "    # t3 = util.normalize_3d_minmax(t3)\n",
    "    \n",
    "\n",
    "    t1 = crop_center(t1, (128, 200, 256),(0, 20, 20))\n",
    "    t2 = crop_center(t2, (128, 200, 256),(0, 20, 20))\n",
    "    t3 = crop_center(t3, (128, 200, 256),(0, 20, 20))\n",
    "    idx = crop_center(idx, (128, 200, 256),(0, 20, 20))\n",
    "\n",
    "   \n",
    "\n",
    "    # t1 = t1/np.mean(t1)\n",
    "    # t2 = t2/np.mean(t2)\n",
    "    # t3 = t3/np.mean(t3)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # t1 = util.normalize_3d_meanstd(t1)\n",
    "    # t2 = util.normalize_3d_meanstd(t2)\n",
    "    # t3 = util.normalize_3d_meanstd(t3)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    d=32\n",
    "    t1_chuncks = make_blocks_vectorized(t1,d)\n",
    "    t2_chuncks = make_blocks_vectorized(t2,d)\n",
    "    t3_chuncks = make_blocks_vectorized(t3,d)\n",
    "    idx_chuncks = make_blocks_vectorized(idx,d)\n",
    "\n",
    "    t1_data.append(t1_chuncks)\n",
    "    t2_data.append(t2_chuncks)\n",
    "    t3_data.append(t3_chuncks)\n",
    "    idx_data.append(idx_chuncks)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# x_data = torch.Tensor(np.stack((np.array(t1_data), np.array(t2_data), np.array(idx_data)),1))\n",
    "t1_data = np.concatenate(t1_data, 0)\n",
    "t2_data = np.concatenate(t2_data, 0)\n",
    "t3_data = np.concatenate(t3_data, 0)\n",
    "idx_data = np.concatenate(idx_data, 0)\n",
    "# x_data = torch.Tensor(np.stack([t1_data, t2_data, idx_data], 4)).permute(0,4,1,2,3)\n",
    "# x_data = torch.Tensor(np.stack([t1_data, t2_data], 4)).permute(0,4,1,2,3)\n",
    "x_data = torch.Tensor(np.stack([t2_data, idx_data], 4)).permute(0,4,1,2,3)\n",
    "#x_data = x_data.unsqueeze(2)\n",
    "y_data = torch.Tensor(t3_data)\n",
    "y_data = y_data.unsqueeze(1)\n",
    "\n",
    "final_dataset = TensorDataset(x_data,y_data) # create your datset\n",
    "torch.save(final_dataset, os.path.join(rootpath, 'data_0907', 'petctdataset_train_chunck.pth'))\n",
    "\n",
    "\n",
    "t1_data = []\n",
    "t2_data = []\n",
    "t3_data = []\n",
    "idx_data = []\n",
    "\n",
    "t1_data_whole = []\n",
    "t2_data_whole = []\n",
    "t3_data_whole = []\n",
    "idx_data_whole = []\n",
    "\n",
    "#patient_ID_list = [1,3,5,7,9,11,14,17, 19,21] # for training\n",
    "#patient_ID_list = [2,4,6,8,10,13,16,18,20] # for testing\n",
    "patient_ID_list = [14,11,2,20,5,19]\n",
    "# patient_ID_list = [2]\n",
    "rootpath = os.path.join(os.path.dirname(os.getcwd()),'python')\n",
    "for patient_ID in  patient_ID_list:\n",
    "    t1_path, t2_path, t3_path, idx_path = path_prep_data(rootpath, patient_ID)\n",
    "\n",
    "    t1 = np.load(t1_path + '.npy')\n",
    "    t2 = np.load(t2_path + '.npy')\n",
    "    t3 = np.load(t3_path + '.npy')\n",
    "    idx = np.load(idx_path + '.npy')\n",
    "\n",
    "    t1[t1<100] =0\n",
    "    t2[t2<100] =0\n",
    "    t3[t3<100] =0\n",
    "\n",
    "    \n",
    "    t1 = t1/np.sum(t1)*1e7\n",
    "    t2 = t2/np.sum(t2)*1e7\n",
    "    t3 = t3/np.sum(t3)*1e7\n",
    "\n",
    "    idx = (idx > 200)\n",
    "    idx = idx.astype(int)\n",
    "\n",
    "    \n",
    "\n",
    "    # t1 = util.normalize_3d_meanstd(t1)\n",
    "    # t2 = util.normalize_3d_meanstd(t2)\n",
    "    # t3 = util.normalize_3d_meanstd(t3)\n",
    "\n",
    "    # t1 = util.normalize_3d_minmax(t1)\n",
    "    # t2 = util.normalize_3d_minmax(t2)\n",
    "    # t3 = util.normalize_3d_minmax(t3)\n",
    "\n",
    "    \n",
    "\n",
    "    t1 = crop_center(t1, (128, 200, 256),(0, 20, 20))\n",
    "    t2 = crop_center(t2, (128, 200, 256),(0, 20, 20))\n",
    "    t3 = crop_center(t3, (128, 200, 256),(0, 20, 20))\n",
    "    idx = crop_center(idx, (128, 200, 256),(0, 20, 20))\n",
    "\n",
    "\n",
    "   \n",
    "    # t1 = t1/np.mean(t1)\n",
    "    # t2 = t2/np.mean(t2)\n",
    "    # t3 = t3/np.mean(t3)\n",
    "\n",
    " \n",
    "    # t1 = util.normalize_3d_meanstd(t1)\n",
    "    # t2 = util.normalize_3d_meanstd(t2)\n",
    "    # t3 = util.normalize_3d_meanstd(t3)\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    # print(np.max(np.max(np.max(t1))))\n",
    "    # print(np.min(np.min(np.min(t1))))\n",
    "\n",
    "    \n",
    "\n",
    "   \n",
    "\n",
    "    # print(np.max(np.max(np.max(t1))))\n",
    "    # print(np.min(np.min(np.min(t1))))\n",
    "\n",
    "    t1_data_whole.append(t1)\n",
    "    t2_data_whole.append(t2)\n",
    "    t3_data_whole.append(t3)\n",
    "    idx_data_whole.append(idx)\n",
    "\n",
    "    d=32\n",
    "    t1_chuncks = make_blocks_vectorized(t1,d)\n",
    "    t2_chuncks = make_blocks_vectorized(t2,d)\n",
    "    t3_chuncks = make_blocks_vectorized(t3,d)\n",
    "    idx_chuncks = make_blocks_vectorized(idx,d)\n",
    "\n",
    "    t1_data.append(t1_chuncks)\n",
    "    t2_data.append(t2_chuncks)\n",
    "    t3_data.append(t3_chuncks)\n",
    "    idx_data.append(idx_chuncks)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# x_data = torch.Tensor(np.stack((np.array(t1_data), np.array(t2_data), np.array(idx_data)),1))\n",
    "t1_data = np.concatenate(t1_data, 0)\n",
    "t2_data = np.concatenate(t2_data, 0)\n",
    "t3_data = np.concatenate(t3_data, 0)\n",
    "idx_data = np.concatenate(idx_data, 0)\n",
    "# x_data = torch.Tensor(np.stack([np.array(t1_data), np.array(t2_data), np.array(idx_data)], 4)).permute(0,4,1,2,3)\n",
    "# x_data = torch.Tensor(np.stack([np.array(t1_data), np.array(t2_data)], 4)).permute(0,4,1,2,3)\n",
    "x_data = torch.Tensor(np.stack([np.array(t2_data), np.array(idx_data)], 4)).permute(0,4,1,2,3)\n",
    "# x_data_whole = torch.Tensor(np.stack([np.array(t1_data_whole), np.array(t2_data_whole), np.array(idx_data_whole)], 4)).permute(0,4,1,2,3)\n",
    "# x_data_whole = torch.Tensor(np.stack([np.array(t1_data_whole), np.array(t2_data_whole)], 4)).permute(0,4,1,2,3)\n",
    "x_data_whole = torch.Tensor(np.stack([np.array(t2_data_whole), np.array(idx_data_whole)], 4)).permute(0,4,1,2,3)\n",
    "#x_data = x_data.unsqueeze(2)\n",
    "y_data = torch.Tensor(t3_data)\n",
    "y_data_whole = torch.Tensor(np.array(t3_data_whole))\n",
    "y_data = y_data.unsqueeze(1)\n",
    "y_data_whole = y_data_whole.unsqueeze(1)\n",
    "\n",
    "final_dataset = TensorDataset(x_data,y_data) # create your datset\n",
    "final_dataset_whole = TensorDataset(x_data_whole,y_data_whole) # create your datset\n",
    "torch.save(final_dataset, os.path.join(rootpath, 'data_0907', 'petctdataset_test_chunck.pth'))\n",
    "torch.save(final_dataset_whole, os.path.join(rootpath, 'data_0907', 'petctdataset_test_whole.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(t3_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a872ac70b6e90aa5c902f72112b61e1b37ed5f2cfbca25f359328a225abc7c82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
